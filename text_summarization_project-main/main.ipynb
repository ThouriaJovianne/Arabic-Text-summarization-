{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìò Welcome to the Arabic Text Summarization Project\n",
    "\n",
    "## Overview\n",
    "Welcome, students üë©‚Äçüéìüë®‚Äçüéì, to our exciting journey into the world of Natural Language Processing (NLP)! In this project, we'll be delving into the fascinating task of text summarization with a focus on the Arabic language üìö. Our goal is to develop a model that can efficiently summarize Arabic text, making it easier to grasp the essence of large documents quickly üöÄ.\n",
    "\n",
    "## Project Objectives\n",
    "- **Understanding Text Summarization**: Learn the fundamentals of how text summarization works üìù.\n",
    "- **Exploring NLP Models**: Get hands-on experience with advanced NLP models like AraGPT2 ü§ñ.\n",
    "- **Model Fine-Tuning and Training**: Discover how to fine-tune pre-trained models on a custom dataset for specific tasks like summarization üß†.\n",
    "- **Practical Application**: Apply your knowledge to build a model that can summarize Arabic texts üåê.\n",
    "\n",
    "## Dataset\n",
    "We'll be using a custom dataset of Arabic texts and their summaries üìñ. This dataset will allow us to train our model to understand and generate concise summaries.\n",
    "\n",
    "We generated this dataset using ChatGPT üòú\n",
    "If you've read this sentence, send me a message.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è **Important: Use GPU Runtime** ‚ö†Ô∏è\n",
    "\n",
    "To ensure this notebook functions correctly and efficiently, it is **crucial to use a GPU runtime**. Follow these steps to enable GPU acceleration:\n",
    "\n",
    "1. **Open Runtime settings**: At the top of the page, click on `Runtime` in the menu bar. üîÑ\n",
    "\n",
    "2. **Change the runtime type**: In the dropdown menu, select `Change runtime type`. üõ†Ô∏è\n",
    "\n",
    "3. **Select GPU as the hardware accelerator**: In the dialog that appears, under `Hardware accelerator`, choose `GPU T4` from the dropdown menu. üñ•Ô∏è\n",
    "\n",
    "4. **Save the settings**: Click `Save` to apply the changes. üíæ\n",
    "\n",
    "By enabling GPU, the computations in this notebook will be significantly faster, especially for tasks like training neural networks, processing large datasets, or performing complex calculations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART1: Load AraGPT2\n",
    "\n",
    "Using the link below, learn how to load araGPT2 base model.\n",
    "\n",
    "https://huggingface.co/aubmindlab/aragpt2-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install arabert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast, pipeline\n",
    "from transformers import GPT2LMHeadModel\n",
    "from arabert.aragpt2.grover.modeling_gpt2 import GPT2LMHeadModel\n",
    "from arabert.preprocess import ArabertPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Complete this cell\n",
    "import torch\n",
    "MODEL_NAME= \"aubmindlab/aragpt2-base\"\n",
    "arabert_prep = ArabertPreprocessor(model_name=MODEL_NAME)\n",
    "\n",
    "\n",
    "text=\"ÿßŸÑÿ¨ÿ≤ÿßÿ¶ÿ± ÿ®ŸÑÿØ\"\n",
    "text_clean = arabert_prep.preprocess(text)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME).to(device)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(MODEL_NAME)\n",
    "generation_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "#feel free to try different decoding settings\n",
    "generation_pipeline(text,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    num_beams=10,\n",
    "    max_length=200,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty = 3.0,\n",
    "    no_repeat_ngram_size = 3)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print AraGPT Model and analyze the architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: print AraGPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART2: Fine-tuning\n",
    "\n",
    "To fine-tune AraGPT2 for text summarization, we use the file `arabic_texts_summaries.csv`\n",
    "\n",
    "#### *Fine-tuning Steps:*\n",
    "\n",
    "\n",
    "1.   Load datasets and split it into train/test\n",
    "2.   Create Datalaoders of train and val.\n",
    "3.   Resize model embeddings for new tokenizer length.\n",
    "4.   Fine-tuning model by passing train data and evaluating it on val data during training.\n",
    "5.   Store the tokenizer and fine-tuned model.\n",
    "6.   Generate summaries for test set which is not used during fine tune.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils_data import * \n",
    "from src.utils_tokenizer import * \n",
    "from src.train import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 254\n",
    "sum_length = 16\n",
    "split_probability = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = process_data(\"data/arabic_texts_summaries.csv\",max_length , sum_length, split_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add token to AraGPT2 tokenizer \n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('aubmindlab/aragpt2-base')\n",
    "\n",
    "special_tokens = {'bos_token':'<BOS>', 'eos_token':'<EOS>', 'pad_token':'<PAD>', 'additional_special_tokens':['<SUMMARIZE>']}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "print('tokenizer len: {}'.format(len(tokenizer)))\n",
    "\t\n",
    "ignore_idx = tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: apply tokenizer \n",
    "import os \n",
    "\n",
    "tokenizer_dir =\"tokenizer_path_save\"\n",
    "if not os.path.exists(tokenizer_dir):\n",
    "  os.makedirs(tokenizer_dir) # Create output directory if needed\n",
    "\n",
    "max_seq_len = 768 \n",
    "tokenizer.save_pretrained(tokenizer_dir)\n",
    "tokenizer_len = len(tokenizer)\n",
    "print('ignore_index: {}'.format(ignore_idx))\n",
    "print('max_len: {}'.format(max_seq_len))\n",
    "\n",
    "train, val, test = tokenize_dataset(tokenizer, train, val, test, max_seq_len) \n",
    "# Fix tokenize_dataset function in utils_tokenizer and call it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate train/val/test files\n",
    "#save tokenized data\n",
    "out_dir=\"tokenizer_data\"\n",
    "processed_set= \"dataset\"\n",
    "data_dir = os.path.join(out_dir, processed_set)\n",
    "if not os.path.exists(data_dir):\n",
    "  os.makedirs(data_dir) # Create output directory if needed\n",
    "file = os.path.join(data_dir,\"train.csv\")\n",
    "train.to_csv(file, index=False)\n",
    "\n",
    "file = os.path.join(data_dir,\"val.csv\")\n",
    "val.to_csv(file, index=False)\n",
    "\n",
    "file = os.path.join(data_dir,\"test.csv\")\n",
    "test.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize train and explain each column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Data Loaders \n",
    "# Fix code in utils_data.py \n",
    "\n",
    "import torch\n",
    "train_dataset, val_dataset= # call function get_gpt2_dataset\n",
    "\t\n",
    "b = train_dataset.__getitem__() # check one data row\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset), batch_size = 1)\n",
    "val_dataloader = DataLoader(val_dataset, sampler = SequentialSampler(val_dataset), batch_size = 1)\n",
    "\n",
    "train_loader_len = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"out_dir\" : \"output_directory\",\n",
    "    \"training_models\": \"training_models_directory\",\n",
    "    \"final_model\": \"final_model_directory\",\n",
    "}\n",
    "\n",
    "# fine tune pretrained model \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_dir =  'aubmindlab/aragpt2-base'\n",
    "\n",
    "train = Train(device, model_dir, tokenizer_len, ignore_idx, train_loader_len, config)\n",
    "train.train_model(train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
